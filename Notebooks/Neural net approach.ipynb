{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net approach\n",
    "\n",
    "The goal of this notebook is to try using neural nets (in particular the [`tabular` nets from `fastai`](https://docs.fast.ai/tutorial.tabular)). While just straight-up using a NN is somewhat problematic in terms of explainability, my goal is just to see what kind of accuracy we can get. If there's no real improvement over what we're seeing with XGB then there's no point fussing with NNs anymore. If there _is_ an improvement, then we should turn our attention to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(train_or_test='train'):\n",
    "    if train_or_test == 'train':\n",
    "        start_date = '2000-01-01'\n",
    "        end_date = '2015-01-01'\n",
    "    else:\n",
    "        start_date = '2015-01-01'\n",
    "        end_date = '2020-01-01'\n",
    "        \n",
    "    ds = Dataset('tf')\n",
    "    ds.load_games(start_date, end_date)\n",
    "    ds.add_team_stats(cols=['Avg_Attendance', 'W-L-pct'])\n",
    "    ds.add_team_pitching_stats(cols=['WHIP', 'ERA'])\n",
    "    ds.add_pitcher_stats(cols=['WHIP', 'ERA', 'IP'], game_offset=5);\n",
    "\n",
    "    ds.data = ds.data.drop(['home_pitcher', 'away_pitcher', 'date'], axis='columns')\n",
    "    \n",
    "    embedding_cols = ['home_team', 'away_team', 'Y', 'M', 'D']\n",
    "    numeric_cols = list(set(ds.data.columns) - set(embedding_cols))\n",
    "    numeric_cols.remove('home_win')\n",
    "    assert set(numeric_cols).intersection(set(embedding_cols)) == set()\n",
    "    assert len(embedding_cols) + len(numeric_cols) + 1 == len(ds.data.columns)\n",
    "    \n",
    "    for c in ds.data.columns:\n",
    "        if ds.data[c].isin([-np.inf, np.inf]).sum() > 0:\n",
    "            ds.data[c] = ds.data[c].replace([-np.inf, np.inf], None)\n",
    "        if ds.data[c].isna().sum() > 0:\n",
    "            med = ds.data[c].median()\n",
    "            ds.data[c] = ds.data[c].fillna(med)\n",
    "            \n",
    "    le = LabelEncoder()\n",
    "    ds.data['away_team'] = le.fit_transform(ds.data['away_team'])\n",
    "    ds.data['home_team'] = le.transform(ds.data['home_team'])\n",
    "    \n",
    "    for c in embedding_cols:\n",
    "        ds.data[c] = ds.data[c].astype(int)\n",
    "        \n",
    "    assert ds.data.isna().sum().sum() == 0\n",
    "    \n",
    "    y = ds.data.pop('home_win')\n",
    "    y = y.astype(int)\n",
    "    X = ds.data\n",
    "    \n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(X), y)).batch(128)\n",
    "    return tf_ds, ds, le, embedding_cols, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_df, le, embedding_cols, numeric_cols = load_ds('train')\n",
    "test_ds, test_df, _, _, _ = load_ds('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for our feature.\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == 'string':\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "    \n",
    "    if name == 'home_team':\n",
    "        print('home_team')\n",
    "        print(index.get_vocabulary())\n",
    "        \n",
    "    if name == 'away_team':\n",
    "        print('away_team')\n",
    "        print(index.get_vocabulary())\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices.\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_columns(dataset, embedding_dims=[10]):\n",
    "    cont_inputs = []\n",
    "    cat_inputs = []\n",
    "    encoded_cont_features = []\n",
    "    encoded_cat_features = []\n",
    "    if isinstance(embedding_dims, int):\n",
    "        embedding_dims = [embedding_dims] * len(embedding_cols)\n",
    "    assert len(embedding_dims) == len(embedding_cols), 'embedding_dims must be an integer or a list with the same length as embedding_cols'\n",
    "\n",
    "    # Numeric features.\n",
    "    print(f'Numerical columns: {numeric_cols}')\n",
    "    for header in numeric_cols:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, dataset)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        encoded_numeric_col = tf.keras.layers.Dropout(0.1)(encoded_numeric_col)\n",
    "        cont_inputs.append(numeric_col)\n",
    "        encoded_cont_features.append(encoded_numeric_col)\n",
    "\n",
    "    # Home and away teams\n",
    "    print(f'Embedding home_team and away_team')\n",
    "    for i, header in enumerate(['home_team', 'away_team']):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(30, embedding_dims[i], name=f'{header}_embedding')(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    # Categorical features encoded as ints.\n",
    "    encoded_embedding_cols = embedding_cols.copy()\n",
    "    encoded_embedding_cols.remove('home_team')\n",
    "    encoded_embedding_cols.remove('away_team')\n",
    "    print(f'Categorical columns: {encoded_embedding_cols}')\n",
    "    for i, header in enumerate(encoded_embedding_cols):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = get_category_encoding_layer(header, dataset, \n",
    "                                                     dtype='int', \n",
    "                                                     max_tokens=20)\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(20, 5, name=f'{header}_embedding')(encoded_categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    all_inputs = cont_inputs + cat_inputs\n",
    "        \n",
    "    return all_inputs, encoded_cont_features, encoded_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['obp_pct_diff', 'team_ERA_pct_diff', 'bayes_pct_diff', 'ops_pct_diff', 'RD_pct_diff', 'R_pct_diff', 'avg_pct_diff', 'pytha_pct_diff', 'team_W-L_pct_diff', 'team_WHIP_pct_diff', 'win_pct_diff', 'team_FP_pct_diff', 'slg_pct_diff', 'RA_pct_diff', 'team_Rank_pct_diff', 'pitcher_WHIP_pct_diff', 'WPA_pct_diff', 'FP_pct_diff', 'Rank_pct_diff', 'pitcher_IP_pct_diff', 'pitcher_ERA_pct_diff', 'log_5']\n",
      "Embedding home_team and away_team\n",
      "Categorical columns: ['Y', 'M']\n"
     ]
    }
   ],
   "source": [
    "all_inputs, encoded_cont_features, encoded_cat_features = prep_columns(train_ds, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.concatenate(encoded_cat_features + encoded_cont_features)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "266/266 [==============================] - 4s 7ms/step - loss: 0.8139 - accuracy: 0.5384 - val_loss: 0.6546 - val_accuracy: 0.5625\n",
      "Epoch 2/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6834 - accuracy: 0.5559 - val_loss: 0.6495 - val_accuracy: 0.5664\n",
      "Epoch 3/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6598 - accuracy: 0.5618 - val_loss: 0.6479 - val_accuracy: 0.5787\n",
      "Epoch 4/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6530 - accuracy: 0.5663 - val_loss: 0.6461 - val_accuracy: 0.5917\n",
      "Epoch 5/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6530 - accuracy: 0.5731 - val_loss: 0.6469 - val_accuracy: 0.5867\n",
      "Epoch 6/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6493 - accuracy: 0.5785 - val_loss: 0.6462 - val_accuracy: 0.5896\n",
      "Epoch 7/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6469 - accuracy: 0.5817 - val_loss: 0.6462 - val_accuracy: 0.5845\n",
      "Epoch 8/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6462 - accuracy: 0.5839 - val_loss: 0.6466 - val_accuracy: 0.5852\n",
      "Epoch 9/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6467 - accuracy: 0.5874 - val_loss: 0.6461 - val_accuracy: 0.5848\n",
      "Epoch 10/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6454 - accuracy: 0.5875 - val_loss: 0.6458 - val_accuracy: 0.5864\n",
      "Epoch 11/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6449 - accuracy: 0.5883 - val_loss: 0.6467 - val_accuracy: 0.5907\n",
      "Epoch 12/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6459 - accuracy: 0.5871 - val_loss: 0.6467 - val_accuracy: 0.5898\n",
      "Epoch 13/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.5930 - val_loss: 0.6468 - val_accuracy: 0.5882\n",
      "Epoch 14/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.5853 - val_loss: 0.6468 - val_accuracy: 0.5891\n",
      "Epoch 15/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6446 - accuracy: 0.5901 - val_loss: 0.6470 - val_accuracy: 0.5870\n",
      "Epoch 16/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6452 - accuracy: 0.5899 - val_loss: 0.6476 - val_accuracy: 0.5858\n",
      "Epoch 17/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6460 - accuracy: 0.5936 - val_loss: 0.6471 - val_accuracy: 0.5878\n",
      "Epoch 18/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6426 - accuracy: 0.5978 - val_loss: 0.6473 - val_accuracy: 0.5874\n",
      "Epoch 19/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6429 - accuracy: 0.5951 - val_loss: 0.6476 - val_accuracy: 0.5886\n",
      "Epoch 20/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.5939 - val_loss: 0.6478 - val_accuracy: 0.5854\n",
      "Epoch 21/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6436 - accuracy: 0.5906 - val_loss: 0.6475 - val_accuracy: 0.5922\n",
      "Epoch 22/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6447 - accuracy: 0.5905 - val_loss: 0.6476 - val_accuracy: 0.5915\n",
      "Epoch 23/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6424 - accuracy: 0.5943 - val_loss: 0.6475 - val_accuracy: 0.5891\n",
      "Epoch 24/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6433 - accuracy: 0.5955 - val_loss: 0.6470 - val_accuracy: 0.5917\n",
      "Epoch 25/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6412 - accuracy: 0.5954 - val_loss: 0.6474 - val_accuracy: 0.5905\n",
      "Epoch 26/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6443 - accuracy: 0.5883 - val_loss: 0.6474 - val_accuracy: 0.5909\n",
      "Epoch 27/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6431 - accuracy: 0.5921 - val_loss: 0.6470 - val_accuracy: 0.5955\n",
      "Epoch 28/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6421 - accuracy: 0.5915 - val_loss: 0.6469 - val_accuracy: 0.5907\n",
      "Epoch 29/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6409 - accuracy: 0.5936 - val_loss: 0.6473 - val_accuracy: 0.5901\n",
      "Epoch 30/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6419 - accuracy: 0.5953 - val_loss: 0.6474 - val_accuracy: 0.5893\n",
      "Epoch 31/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6421 - accuracy: 0.5952 - val_loss: 0.6473 - val_accuracy: 0.5919\n",
      "Epoch 32/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6416 - accuracy: 0.5970 - val_loss: 0.6475 - val_accuracy: 0.5892\n",
      "Epoch 33/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6422 - accuracy: 0.5936 - val_loss: 0.6476 - val_accuracy: 0.5903\n",
      "Epoch 34/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6396 - accuracy: 0.5985 - val_loss: 0.6474 - val_accuracy: 0.5904\n",
      "Epoch 35/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6391 - accuracy: 0.6012 - val_loss: 0.6476 - val_accuracy: 0.5914\n",
      "Epoch 36/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6403 - accuracy: 0.5993 - val_loss: 0.6477 - val_accuracy: 0.5946\n",
      "Epoch 37/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.6028 - val_loss: 0.6482 - val_accuracy: 0.5965\n",
      "Epoch 38/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6386 - accuracy: 0.6025 - val_loss: 0.6478 - val_accuracy: 0.5937\n",
      "Epoch 39/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.5990 - val_loss: 0.6479 - val_accuracy: 0.5914\n",
      "Epoch 40/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6408 - accuracy: 0.5973 - val_loss: 0.6479 - val_accuracy: 0.5990\n",
      "Epoch 41/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6397 - accuracy: 0.5999 - val_loss: 0.6481 - val_accuracy: 0.5958\n",
      "Epoch 42/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6394 - accuracy: 0.6034 - val_loss: 0.6483 - val_accuracy: 0.5962\n",
      "Epoch 43/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6389 - accuracy: 0.6035 - val_loss: 0.6494 - val_accuracy: 0.5977\n",
      "Epoch 44/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.6015 - val_loss: 0.6480 - val_accuracy: 0.5922\n",
      "Epoch 45/50\n",
      "266/266 [==============================] - 2s 6ms/step - loss: 0.6368 - accuracy: 0.6053 - val_loss: 0.6494 - val_accuracy: 0.5939\n",
      "Epoch 46/50\n",
      "266/266 [==============================] - 1s 6ms/step - loss: 0.6388 - accuracy: 0.6019 - val_loss: 0.6488 - val_accuracy: 0.5953\n",
      "Epoch 47/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6378 - accuracy: 0.6018 - val_loss: 0.6486 - val_accuracy: 0.5963\n",
      "Epoch 48/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6362 - accuracy: 0.6042 - val_loss: 0.6491 - val_accuracy: 0.5966\n",
      "Epoch 49/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.6057 - val_loss: 0.6497 - val_accuracy: 0.5957\n",
      "Epoch 50/50\n",
      "266/266 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.6091 - val_loss: 0.6494 - val_accuracy: 0.5949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167345eb0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 0s 5ms/step - loss: 0.6557 - accuracy: 0.6015\n",
      "Test accuracy = 60.15%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f'Test accuracy = {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Get the team embeddings from the embedding layers. Following [this tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team_weights = model.get_layer('home_team_embedding').get_weights()[0]\n",
    "away_team_weights = model.get_layer('away_team_embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 10)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_team_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def save_embeddings(home_or_away, vocab, weights):\n",
    "    out_v = io.open(f'{home_or_away}_team_vectors.tsv', 'w', encoding='utf-8')\n",
    "    out_m = io.open(f'{home_or_away}_team_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "    for index, word in enumerate(vocab):\n",
    "        vec = weights[index] \n",
    "        out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "        out_m.write(f'{word}\\n')\n",
    "    out_v.close()\n",
    "    out_m.close()\n",
    "    \n",
    "    return dict(zip(vocab, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_mapping = save_embeddings('home', vocab, home_team_weights)\n",
    "away_mapping = save_embeddings('away', vocab, away_team_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_top5 = dict()\n",
    "\n",
    "for t1 in vocab:\n",
    "    t1_distances = []\n",
    "    for t2 in vocab:\n",
    "        if t1 != t2:\n",
    "            t1_weights = home_mapping[t1]\n",
    "            t2_weights = home_mapping[t2]\n",
    "            t1_distances.append((t2, cosine_similarity(t1_weights, t2_weights)))\n",
    "    t1_distances = sorted(t1_distances, key=lambda x: x[-1])\n",
    "    team_top5[t1] = [x[0] for x in t1_distances[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ANA': ['SFN', 'SEA', 'LAN', 'WAS', 'SLN'],\n",
       " 'ARI': ['WAS', 'HOU', 'BOS', 'MIL', 'MIA'],\n",
       " 'ATL': ['TBA', 'DET', 'ANA', 'SLN', 'COL'],\n",
       " 'BAL': ['MIN', 'KCA', 'SDN', 'CLE', 'PHI'],\n",
       " 'BOS': ['MIL', 'ARI', 'WAS', 'LAN', 'MIA'],\n",
       " 'CHA': ['ARI', 'PIT', 'SLN', 'COL', 'WAS'],\n",
       " 'CHN': ['PIT', 'MIL', 'TOR', 'TBA', 'CIN'],\n",
       " 'CIN': ['KCA', 'TBA', 'TEX', 'CHN', 'DET'],\n",
       " 'CLE': ['TEX', 'SDN', 'NYN', 'BAL', 'PHI'],\n",
       " 'COL': ['TOR', 'CHA', 'TBA', 'SLN', 'ATL'],\n",
       " 'DET': ['ATL', 'NYN', 'TBA', 'TEX', 'CIN'],\n",
       " 'HOU': ['BOS', 'BAL', 'ARI', 'MIA', 'MIN'],\n",
       " 'KCA': ['CIN', 'PHI', 'BAL', 'TEX', 'NYN'],\n",
       " 'LAN': ['MIA', 'ANA', 'BOS', 'SFN', 'WAS'],\n",
       " 'MIA': ['MIL', 'LAN', 'WAS', 'ARI', 'BOS'],\n",
       " 'MIL': ['CHA', 'CHN', 'MIA', 'BOS', 'ARI'],\n",
       " 'MIN': ['NYA', 'MIA', 'BAL', 'OAK', 'HOU'],\n",
       " 'NYA': ['ATL', 'SEA', 'SLN', 'SFN', 'OAK'],\n",
       " 'NYN': ['DET', 'SDN', 'PHI', 'KCA', 'TEX'],\n",
       " 'OAK': ['SFN', 'ANA', 'MIN', 'SEA', 'NYA'],\n",
       " 'PHI': ['KCA', 'TEX', 'NYN', 'CLE', 'BAL'],\n",
       " 'PIT': ['SDN', 'ARI', 'CHN', 'CHA', 'TOR'],\n",
       " 'SDN': ['PIT', 'PHI', 'BAL', 'TEX', 'NYN'],\n",
       " 'SEA': ['SLN', 'NYA', 'SFN', 'ANA', 'OAK'],\n",
       " 'SFN': ['SLN', 'ANA', 'NYA', 'WAS', 'LAN'],\n",
       " 'SLN': ['SFN', 'ATL', 'WAS', 'COL', 'ANA'],\n",
       " 'TBA': ['CIN', 'DET', 'CHN', 'COL', 'TOR'],\n",
       " 'TEX': ['PHI', 'SDN', 'KCA', 'DET', 'NYN'],\n",
       " 'TOR': ['DET', 'PIT', 'CHN', 'COL', 'TBA'],\n",
       " 'WAS': ['BOS', 'SLN', 'ANA', 'CHA', 'LAN']}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_2016_test_df = test_df.data[(test_df.data['home_team'] == le.transform(['ANA'])[0]) & (test_df.data['Y'] == 2016)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>M</th>\n",
       "      <th>D</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_elo</th>\n",
       "      <th>away_elo</th>\n",
       "      <th>home_avg</th>\n",
       "      <th>away_avg</th>\n",
       "      <th>home_obp</th>\n",
       "      <th>...</th>\n",
       "      <th>away_WHIP_offset1year</th>\n",
       "      <th>away_ERA_offset1year</th>\n",
       "      <th>home_pitcher_season_game</th>\n",
       "      <th>home_pitcher_WHIP_avg_5games</th>\n",
       "      <th>home_pitcher_ERA_avg_5games</th>\n",
       "      <th>home_pitcher_IP_avg_5games</th>\n",
       "      <th>away_pitcher_season_game</th>\n",
       "      <th>away_pitcher_WHIP_avg_5games</th>\n",
       "      <th>away_pitcher_ERA_avg_5games</th>\n",
       "      <th>away_pitcher_IP_avg_5games</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1509.867554</td>\n",
       "      <td>1526.674805</td>\n",
       "      <td>0.240166</td>\n",
       "      <td>0.239208</td>\n",
       "      <td>0.30189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151872</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.381221</td>\n",
       "      <td>3.826</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.382767</td>\n",
       "      <td>3.828</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1505.534912</td>\n",
       "      <td>1531.007324</td>\n",
       "      <td>0.240166</td>\n",
       "      <td>0.239208</td>\n",
       "      <td>0.30189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151872</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.381221</td>\n",
       "      <td>3.826</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.382767</td>\n",
       "      <td>3.828</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1502.591187</td>\n",
       "      <td>1510.292358</td>\n",
       "      <td>0.240166</td>\n",
       "      <td>0.249648</td>\n",
       "      <td>0.30189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363889</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.381221</td>\n",
       "      <td>3.826</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.382767</td>\n",
       "      <td>3.828</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1503.895264</td>\n",
       "      <td>1508.988159</td>\n",
       "      <td>0.240166</td>\n",
       "      <td>0.249648</td>\n",
       "      <td>0.30189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363889</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.381221</td>\n",
       "      <td>3.826</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.382767</td>\n",
       "      <td>3.828</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1501.109863</td>\n",
       "      <td>1511.773560</td>\n",
       "      <td>0.240166</td>\n",
       "      <td>0.249648</td>\n",
       "      <td>0.30189</td>\n",
       "      <td>...</td>\n",
       "      <td>1.363889</td>\n",
       "      <td>4.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>5.400</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.382767</td>\n",
       "      <td>3.828</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Y  M  D  home_team  away_team     home_elo     away_elo  home_avg  \\\n",
       "2439  2016  4  4          0          6  1509.867554  1526.674805  0.240166   \n",
       "2448  2016  4  5          0          6  1505.534912  1531.007324  0.240166   \n",
       "2469  2016  4  7          0         27  1502.591187  1510.292358  0.240166   \n",
       "2482  2016  4  8          0         27  1503.895264  1508.988159  0.240166   \n",
       "2496  2016  4  9          0         27  1501.109863  1511.773560  0.240166   \n",
       "\n",
       "      away_avg  home_obp  ...  away_WHIP_offset1year  away_ERA_offset1year  \\\n",
       "2439  0.239208   0.30189  ...               1.151872                  3.36   \n",
       "2448  0.239208   0.30189  ...               1.151872                  3.36   \n",
       "2469  0.249648   0.30189  ...               1.363889                  4.25   \n",
       "2482  0.249648   0.30189  ...               1.363889                  4.25   \n",
       "2496  0.249648   0.30189  ...               1.363889                  4.25   \n",
       "\n",
       "      home_pitcher_season_game  home_pitcher_WHIP_avg_5games  \\\n",
       "2439                       1.0                      1.381221   \n",
       "2448                       1.0                      1.381221   \n",
       "2469                       1.0                      1.381221   \n",
       "2482                       1.0                      1.381221   \n",
       "2496                       2.0                      1.800000   \n",
       "\n",
       "      home_pitcher_ERA_avg_5games  home_pitcher_IP_avg_5games  \\\n",
       "2439                        3.826                         5.6   \n",
       "2448                        3.826                         5.6   \n",
       "2469                        3.826                         5.6   \n",
       "2482                        3.826                         5.6   \n",
       "2496                        5.400                         5.0   \n",
       "\n",
       "      away_pitcher_season_game  away_pitcher_WHIP_avg_5games  \\\n",
       "2439                       1.0                      1.382767   \n",
       "2448                       1.0                      1.382767   \n",
       "2469                       1.0                      1.382767   \n",
       "2482                       1.0                      1.382767   \n",
       "2496                       1.0                      1.382767   \n",
       "\n",
       "      away_pitcher_ERA_avg_5games  away_pitcher_IP_avg_5games  \n",
       "2439                        3.828                         5.6  \n",
       "2448                        3.828                         5.6  \n",
       "2469                        3.828                         5.6  \n",
       "2482                        3.828                         5.6  \n",
       "2496                        3.828                         5.6  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana_2016_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
