{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization\n",
    "\n",
    "The point of this notebook is to use the [Bayesian optimization package](https://github.com/fmfn/BayesianOptimization) to do an intelligent hyperparameter search for XGB. In this notebook we'll run hyperparameter tuning on the XGB model. I'm following both the documentation on the package github page, along with [this Kaggle tutorial](https://www.kaggle.com/tilii7/bayesian-optimization-of-xgboost-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/start_to_finish.csv')  #Pct Diff Columns Only (Gives Highest Accuracy)\n",
    "#df = pd.read_csv('../data/Final Data/diff-mlb-games.csv')    #Diff columns only\n",
    "#df = pd.read_csv('../data/Final Data/full-diff-mlb-games.csv')    #All columns\n",
    "\n",
    "cols = ['team_ops_pct_diff', 'obp_diff', 'team_obp_pct_diff',\n",
    "       'home_Rank_offset1year', 'away_WHIP_offset1year', 'team_ERA_pct_diff',\n",
    "       'home_win_diff_bayes', 'home_RD', 'team_bayes_pct_diff',\n",
    "       'away_win_diff_bayes', 'team_RA_pct_diff', 'team_slg_pct_diff',\n",
    "       'team_W-L_pct_diff', 'away_ERA_offset1year', 'away_win_pct',\n",
    "       'away_pitcher_IP_avg_162games', 'team_Rank_pct_diff',\n",
    "       'home_pitcher_IP_avg_162games', 'away_RD',\n",
    "       'home_pitcher_WHIP_avg_162games', 'team_WHIP_pct_diff',\n",
    "       'home_bayes_win', 'home_win_diff', 'pitcher_IP_pct_diff',\n",
    "       'away_Rank_offset1year', 'home_total_R', 'home_pythag_expect',\n",
    "       'home_obp', 'team_R_pct_diff', 'home_avg',\n",
    "       'away_pitcher_WPA_avg_162games', 'home_ops', 'away_pitcher_season_game',\n",
    "       'avg_diff', 'team_avg_pct_diff', 'home_W-L-pct_offset1year',\n",
    "       'home_ERA_offset1year', 'away_R_offset1year', 'home_win_pct',\n",
    "       'away_elo']\n",
    "\n",
    "train_df = df[df['Y'] <= 2015][cols + ['home_win']] \n",
    "test_df = df[df['Y'] > 2015][cols + ['home_win']]\n",
    "\n",
    "X_train = train_df.drop('home_win', axis=1)\n",
    "y_train = train_df.home_win\n",
    "\n",
    "X_test = test_df.drop('home_win', axis=1)\n",
    "y_test = test_df.home_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open('AUC-5fold-XGB.log', 'a')\n",
    "AUCbest = -1.0\n",
    "ITERbest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(max_depth, min_child_weight, eta, subsample, colsample_bytree, gamma):\n",
    "    global AUCbest\n",
    "    global ITERbest\n",
    "    \n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'eta': eta,\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'gamma': gamma,\n",
    "              'seed': 0,\n",
    "              'nthread': 4,\n",
    "              'objective': 'binary:logistic',\n",
    "              'eval_metric': 'auc'}\n",
    "    \n",
    "    folds = 5\n",
    "    cv_score = 0\n",
    "    \n",
    "    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, params), file=log_file)\n",
    "    log_file.flush()\n",
    "\n",
    "    xgbc = xgb.cv(\n",
    "                    params,\n",
    "                    dtrain,\n",
    "                    num_boost_round = 20000,\n",
    "                    stratified = True,\n",
    "                    nfold = folds,\n",
    "                    early_stopping_rounds = 100,\n",
    "                    metrics = 'auc',\n",
    "                    show_stdv = True\n",
    "               )\n",
    "    \n",
    "    val_score = xgbc['test-auc-mean'].iloc[-1]\n",
    "    train_score = xgbc['train-auc-mean'].iloc[-1]\n",
    "    print('Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n",
    "(val_score*2-1)))\n",
    "    if val_score > AUCbest:\n",
    "        AUCbest = val_score\n",
    "        ITERbest = len(xgbc)\n",
    "\n",
    "    return (val_score*2) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': (3, 20),\n",
    "          'min_child_weight': (0.001, 10),\n",
    "          'eta': (0.001, 1.0),\n",
    "          'subsample': (0.6, 1.0),\n",
    "          'colsample_bytree': (0.6, 1.0),\n",
    "          'gamma': (0.001, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_BO = BayesianOptimization(xgb_cv, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | min_ch... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Stopped after 1 iterations with train-auc = 0.686580 val-auc = 0.634804 ( diff = 0.051777 ) train-gini = 0.373160 val-gini = 0.269607\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.2696  \u001b[0m | \u001b[0m 0.8728  \u001b[0m | \u001b[0m 0.8267  \u001b[0m | \u001b[0m 8.736   \u001b[0m | \u001b[0m 9.238   \u001b[0m | \u001b[0m 3.199   \u001b[0m | \u001b[0m 0.6695  \u001b[0m |\n",
      "Stopped after 3 iterations with train-auc = 0.696259 val-auc = 0.654165 ( diff = 0.042094 ) train-gini = 0.392518 val-gini = 0.308330\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.3083  \u001b[0m | \u001b[95m 0.9681  \u001b[0m | \u001b[95m 0.6908  \u001b[0m | \u001b[95m 6.27    \u001b[0m | \u001b[95m 6.52    \u001b[0m | \u001b[95m 7.212   \u001b[0m | \u001b[95m 0.8479  \u001b[0m |\n",
      "Stopped after 7 iterations with train-auc = 0.899019 val-auc = 0.620986 ( diff = 0.278033 ) train-gini = 0.798039 val-gini = 0.241972\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.242   \u001b[0m | \u001b[0m 0.616   \u001b[0m | \u001b[0m 0.4727  \u001b[0m | \u001b[0m 2.919   \u001b[0m | \u001b[0m 18.57   \u001b[0m | \u001b[0m 8.293   \u001b[0m | \u001b[0m 0.8682  \u001b[0m |\n",
      "Stopped after 2 iterations with train-auc = 0.684358 val-auc = 0.651636 ( diff = 0.032722 ) train-gini = 0.368717 val-gini = 0.303273\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 0.6227  \u001b[0m | \u001b[0m 6.158   \u001b[0m | \u001b[0m 6.697   \u001b[0m | \u001b[0m 7.464   \u001b[0m | \u001b[0m 0.6768  \u001b[0m |\n",
      "Stopped after 4 iterations with train-auc = 0.671490 val-auc = 0.658872 ( diff = 0.012618 ) train-gini = 0.342980 val-gini = 0.317744\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.3177  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 6.426   \u001b[0m | \u001b[95m 3.801   \u001b[0m | \u001b[95m 6.292   \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "XGB_BO.maximize(init_points=2, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'max_depth': int(XGB_BO.max['params']['max_depth']),\n",
    "              'min_child_weight': XGB_BO.max['params']['min_child_weight'],\n",
    "              'eta': XGB_BO.max['params']['eta'],\n",
    "              'subsample': XGB_BO.max['params']['subsample'],\n",
    "              'colsample_bytree': XGB_BO.max['params']['colsample_bytree'],\n",
    "              'gamma': XGB_BO.max['params']['gamma'],\n",
    "              'seed': 0,\n",
    "              'nthread': 4,\n",
    "              'objective': 'binary:logistic',\n",
    "              'eval_metric': 'auc'}\n",
    "\n",
    "xgb_best = xgb.train(best_params, dtrain, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3,\n",
       " 'min_child_weight': 6.292249181210905,\n",
       " 'eta': 1.0,\n",
       " 'subsample': 1.0,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'gamma': 6.4258711159875235,\n",
       " 'seed': 0,\n",
       " 'nthread': 4,\n",
       " 'objective': 'binary:logistic',\n",
       " 'eval_metric': 'auc'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_proba = xgb_best.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.round(test_preds_proba, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.55      0.58      4656\n",
      "         1.0       0.63      0.69      0.66      5282\n",
      "\n",
      "    accuracy                           0.62      9938\n",
      "   macro avg       0.62      0.62      0.62      9938\n",
      "weighted avg       0.62      0.62      0.62      9938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.622358623465486"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
