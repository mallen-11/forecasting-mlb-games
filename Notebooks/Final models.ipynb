{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final models\n",
    "\n",
    "The purpose of this notebook is to collect \"final versions\" of the models we want to use for ensembling. These will all use the same data (though they can certainly omit columns if you want). The point is to go from zero-to-trained model in one notebook so we can make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, concatenate\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Load the data here. Models below can augment it if need-be (for example the neural net needs it as a tensorflow dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Final Data/pct-diff-mlb-games.csv')  #Pct Diff Columns Only (Gives Highest Accuracy)\n",
    "#df = pd.read_csv('../data/Final Data/diff-mlb-games.csv')    #Diff columns only\n",
    "#df = pd.read_csv('../data/Final Data/full-diff-mlb-games.csv')    #All columns\n",
    "\n",
    "train_df = df[df['Y'] <= 2015]\n",
    "test_df = df[df['Y'] > 2015]\n",
    "\n",
    "X_train = train_df.drop('home_win', axis=1)\n",
    "y_train = train_df.home_win\n",
    "\n",
    "X_test = test_df.drop('home_win', axis=1)\n",
    "y_test = test_df.home_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "xgbmodel = model.fit(X_train, y_train)\n",
    "\n",
    "xgbpreds = xgbmodel.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, xgbpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgbpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredf = pd.DataFrame(xgbmodel.feature_importances_, X_train.columns)   #This makes it in order\n",
    "featuredf = featuredf.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "x = featuredf.index\n",
    "y = featuredf[0]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importances', fontsize=20)\n",
    "plt.ylabel('Weight', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy By Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_accuracy(model)\n",
    "    monthly_acc = []\n",
    "    months_list = np.sort(X_test['M'].unique())\n",
    "    for months in months_list:\n",
    "        test_month = test[test['M'] == months]\n",
    "\n",
    "        X_test_month = test_month.drop('home_win', axis=1)\n",
    "        y_test_month = test_month.home_win\n",
    "\n",
    "        pred = model.predict(X_test_month)\n",
    "        acc = accuracy_score(y_test_month, pred)\n",
    "\n",
    "        monthly_acc.append(acc)\n",
    "\n",
    "    monthly_acc = pd.DataFrame(monthly_acc)\n",
    "    \n",
    "    return monthly_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_month = monthly_accuracy(xgbmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(xgb_month.index, xgb_month[0]*100, alpha=0.75)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xticks([0,1,2,3,4,5,6,7], ['March','April', 'May', 'June','July', 'August', 'September', 'October']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf = KNeighborsClassifier()\n",
    "nn_clf.fit(X_train, y_train)\n",
    "\n",
    "nn_preds = nn_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, nn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_month = monthly_accuracy(nn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(nn_month.index, nn_month[0]*100, alpha=0.75)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xticks([0,1,2,3,4,5,6,7], ['March','April', 'May', 'June','July', 'August', 'September', 'October']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tensorflow dataset from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(df):\n",
    "    # TODO: Modify this for the incoming data\n",
    "    df = df.drop(['home_pitcher', 'away_pitcher', 'date'], axis='columns')\n",
    "    \n",
    "    embedding_cols = ['home_team', 'away_team', 'Y', 'M', 'D']\n",
    "    numeric_cols = list(set(df.columns) - set(embedding_cols))\n",
    "    numeric_cols.remove('home_win')\n",
    "    assert set(numeric_cols).intersection(set(embedding_cols)) == set()\n",
    "    assert len(embedding_cols) + len(numeric_cols) + 1 == len(df.columns)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if df[c].isin([-np.inf, np.inf]).sum() > 0:\n",
    "            df[c] = df[c].replace([-np.inf, np.inf], None)\n",
    "        if df[c].isna().sum() > 0:\n",
    "            med = df[c].median()\n",
    "            df[c] = df[c].fillna(med)\n",
    "            \n",
    "    le = LabelEncoder()\n",
    "    df['away_team'] = le.fit_transform(dsf['away_team'])\n",
    "    df['home_team'] = le.transform(dsf['home_team'])\n",
    "    \n",
    "    for c in embedding_cols:\n",
    "        df[c] = df[c].astype(int)\n",
    "        \n",
    "    assert df.isna().sum().sum() == 0\n",
    "    \n",
    "    y = df.pop('home_win')\n",
    "    y = y.astype(int)\n",
    "    X = df\n",
    "    \n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(X), y)).batch(128)\n",
    "    return tf_ds, ds, le, embedding_cols, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_ds(train_df)\n",
    "test_ds = load_ds(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for our feature.\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == 'string':\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "    \n",
    "    if name == 'home_team':\n",
    "        print('home_team')\n",
    "        print(index.get_vocabulary())\n",
    "        \n",
    "    if name == 'away_team':\n",
    "        print('away_team')\n",
    "        print(index.get_vocabulary())\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices.\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "def prep_columns(dataset, embedding_dims=[10]):\n",
    "    cont_inputs = []\n",
    "    cat_inputs = []\n",
    "    encoded_cont_features = []\n",
    "    encoded_cat_features = []\n",
    "    if isinstance(embedding_dims, int):\n",
    "        embedding_dims = [embedding_dims] * len(embedding_cols)\n",
    "    assert len(embedding_dims) == len(embedding_cols), 'embedding_dims must be an integer or a list with the same length as embedding_cols'\n",
    "\n",
    "    # Numeric features.\n",
    "    print(f'Numerical columns: {numeric_cols}')\n",
    "    for header in numeric_cols:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, dataset)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        encoded_numeric_col = tf.keras.layers.Dropout(0.1)(encoded_numeric_col)\n",
    "        cont_inputs.append(numeric_col)\n",
    "        encoded_cont_features.append(encoded_numeric_col)\n",
    "\n",
    "    # Home and away teams\n",
    "    print(f'Embedding home_team and away_team')\n",
    "    for i, header in enumerate(['home_team', 'away_team']):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(30, embedding_dims[i], name=f'{header}_embedding')(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    # Categorical features encoded as ints.\n",
    "    encoded_embedding_cols = embedding_cols.copy()\n",
    "    encoded_embedding_cols.remove('home_team')\n",
    "    encoded_embedding_cols.remove('away_team')\n",
    "    print(f'Categorical columns: {encoded_embedding_cols}')\n",
    "    for i, header in enumerate(encoded_embedding_cols):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = get_category_encoding_layer(header, dataset, \n",
    "                                                     dtype='int', \n",
    "                                                     max_tokens=20)\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(20, 5, name=f'{header}_embedding')(encoded_categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    all_inputs = cont_inputs + cat_inputs\n",
    "        \n",
    "    return all_inputs, encoded_cont_features, encoded_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs, encoded_cont_features, encoded_cat_features = prep_columns(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenate(encoded_cat_features + encoded_cont_features)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1)(x)\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f'Test accuracy = {100*acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
