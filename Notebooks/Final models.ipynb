{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final models\n",
    "\n",
    "The purpose of this notebook is to collect \"final versions\" of the models we want to use for ensembling. These will all use the same data (though they can certainly omit columns if you want). The point is to go from zero-to-trained model in one notebook so we can make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, concatenate\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Load the data here. Models below can augment it if need-be (for example the neural net needs it as a tensorflow dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(None)\n",
    "test_df = pd.read_csv(None)\n",
    "\n",
    "X_train = None\n",
    "y_train = None\n",
    "\n",
    "X_test = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for our feature.\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == 'string':\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "    \n",
    "    if name == 'home_team':\n",
    "        print('home_team')\n",
    "        print(index.get_vocabulary())\n",
    "        \n",
    "    if name == 'away_team':\n",
    "        print('away_team')\n",
    "        print(index.get_vocabulary())\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices.\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "def prep_columns(dataset, embedding_dims=[10]):\n",
    "    cont_inputs = []\n",
    "    cat_inputs = []\n",
    "    encoded_cont_features = []\n",
    "    encoded_cat_features = []\n",
    "    if isinstance(embedding_dims, int):\n",
    "        embedding_dims = [embedding_dims] * len(embedding_cols)\n",
    "    assert len(embedding_dims) == len(embedding_cols), 'embedding_dims must be an integer or a list with the same length as embedding_cols'\n",
    "\n",
    "    # Numeric features.\n",
    "    print(f'Numerical columns: {numeric_cols}')\n",
    "    for header in numeric_cols:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, dataset)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        encoded_numeric_col = tf.keras.layers.Dropout(0.1)(encoded_numeric_col)\n",
    "        cont_inputs.append(numeric_col)\n",
    "        encoded_cont_features.append(encoded_numeric_col)\n",
    "\n",
    "    # Home and away teams\n",
    "    print(f'Embedding home_team and away_team')\n",
    "    for i, header in enumerate(['home_team', 'away_team']):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(30, embedding_dims[i], name=f'{header}_embedding')(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    # Categorical features encoded as ints.\n",
    "    encoded_embedding_cols = embedding_cols.copy()\n",
    "    encoded_embedding_cols.remove('home_team')\n",
    "    encoded_embedding_cols.remove('away_team')\n",
    "    print(f'Categorical columns: {encoded_embedding_cols}')\n",
    "    for i, header in enumerate(encoded_embedding_cols):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = get_category_encoding_layer(header, dataset, \n",
    "                                                     dtype='int', \n",
    "                                                     max_tokens=20)\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(20, 5, name=f'{header}_embedding')(encoded_categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    all_inputs = cont_inputs + cat_inputs\n",
    "        \n",
    "    return all_inputs, encoded_cont_features, encoded_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs, encoded_cont_features, encoded_cat_features = prep_columns(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenate(encoded_cat_features + encoded_cont_features)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1)(x)\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f'Test accuracy = {100*acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
