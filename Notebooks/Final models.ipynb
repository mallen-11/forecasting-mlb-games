{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final models\n",
    "\n",
    "The purpose of this notebook is to collect \"final versions\" of the models we want to use for ensembling. These will all use the same data (though they can certainly omit columns if you want). The point is to go from zero-to-trained model in one notebook so we can make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, concatenate\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Load the data here. Models below can augment it if need-be (for example the neural net needs it as a tensorflow dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Final Data/pct-diff-mlb-games.csv')  #Pct Diff Columns Only (Gives Highest Accuracy)\n",
    "#df = pd.read_csv('../data/Final Data/diff-mlb-games.csv')    #Diff columns only\n",
    "#df = pd.read_csv('../data/Final Data/full-diff-mlb-games.csv')    #All columns\n",
    "\n",
    "train_df = df[df['Y'] <= 2015]\n",
    "test_df = df[df['Y'] > 2015]\n",
    "\n",
    "X_train = train_df.drop('home_win', axis=1)\n",
    "y_train = train_df.home_win\n",
    "\n",
    "X_test = test_df.drop('home_win', axis=1)\n",
    "y_test = test_df.home_win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "xgbmodel = model.fit(X_train, y_train)\n",
    "\n",
    "xgbpreds = xgbmodel.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, xgbpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, xgbpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuredf = pd.DataFrame(xgbmodel.feature_importances_, X_train.columns)   #This makes it in order\n",
    "featuredf = featuredf.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "x = featuredf.index\n",
    "y = featuredf[0]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importances', fontsize=20)\n",
    "plt.ylabel('Weight', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy By Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_accuracy(model)\n",
    "    monthly_acc = []\n",
    "    months_list = np.sort(X_test['M'].unique())\n",
    "    for months in months_list:\n",
    "        test_month = test[test['M'] == months]\n",
    "\n",
    "        X_test_month = test_month.drop('home_win', axis=1)\n",
    "        y_test_month = test_month.home_win\n",
    "\n",
    "        pred = model.predict(X_test_month)\n",
    "        acc = accuracy_score(y_test_month, pred)\n",
    "\n",
    "        monthly_acc.append(acc)\n",
    "\n",
    "    monthly_acc = pd.DataFrame(monthly_acc)\n",
    "    \n",
    "    return monthly_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_month = monthly_accuracy(xgbmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(xgb_month.index, xgb_month[0]*100, alpha=0.75)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xticks([0,1,2,3,4,5,6,7], ['March','April', 'May', 'June','July', 'August', 'September', 'October']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf = KNeighborsClassifier()\n",
    "nn_clf.fit(X_train, y_train)\n",
    "\n",
    "nn_preds = nn_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, nn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, nn_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_month = monthly_accuracy(nn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(nn_month.index, nn_month[0]*100, alpha=0.75)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xticks([0,1,2,3,4,5,6,7], ['March','April', 'May', 'June','July', 'August', 'September', 'October']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tensorflow dataset from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(df):\n",
    "    cols_to_drop = ['home_pitcher', 'away_pitcher', 'date']\n",
    "    cols_to_drop = list(set(df.columns).intersection(set(cols_to_drop)))\n",
    "    df = df.drop(cols_to_drop, axis='columns')\n",
    "    \n",
    "    embedding_cols = ['home_team', 'away_team', 'Y', 'M']\n",
    "    numeric_cols = list(set(df.columns) - set(embedding_cols))\n",
    "    numeric_cols.remove('home_win')\n",
    "    assert set(numeric_cols).intersection(set(embedding_cols)) == set()\n",
    "    assert len(embedding_cols) + len(numeric_cols) + 1 == len(df.columns)\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if df[c].isin([-np.inf, np.inf]).sum() > 0:\n",
    "            df[c] = df[c].replace([-np.inf, np.inf], None)\n",
    "        if df[c].isna().sum() > 0:\n",
    "            med = df[c].median()\n",
    "            df[c] = df[c].fillna(med)\n",
    "            \n",
    "    le = LabelEncoder()\n",
    "    df['away_team'] = le.fit_transform(df['away_team'])\n",
    "    df['home_team'] = le.transform(df['home_team'])\n",
    "    \n",
    "    for c in embedding_cols:\n",
    "        df[c] = df[c].astype(int)\n",
    "        \n",
    "    assert df.isna().sum().sum() == 0\n",
    "    \n",
    "    y = df.pop('home_win')\n",
    "    y = y.astype(int)\n",
    "    X = df\n",
    "    \n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(X), y)).batch(128)\n",
    "    return tf_ds, df, le, embedding_cols, numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_df, le, embedding_cols, numeric_cols = load_ds(train_df)\n",
    "test_ds, test_df, _, _, _ = load_ds(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "    # Create a Normalization layer for our feature.\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data.\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    if dtype == 'string':\n",
    "        index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "    else:\n",
    "        index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the set of possible values and assign them a fixed integer index.\n",
    "    index.adapt(feature_ds)\n",
    "    \n",
    "    if name == 'home_team':\n",
    "        print('home_team')\n",
    "        print(index.get_vocabulary())\n",
    "        \n",
    "    if name == 'away_team':\n",
    "        print('away_team')\n",
    "        print(index.get_vocabulary())\n",
    "\n",
    "    # Create a Discretization for our integer indices.\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature.\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices.\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "    # layer so we can use them, or include them in the functional model later.\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "def prep_columns(dataset, embedding_dims=10, embedding_cols=None, numeric_cols=None):\n",
    "    cont_inputs = []\n",
    "    cat_inputs = []\n",
    "    encoded_cont_features = []\n",
    "    encoded_cat_features = []\n",
    "    if isinstance(embedding_dims, int):\n",
    "        embedding_dims = [embedding_dims] * len(embedding_cols)\n",
    "    assert len(embedding_dims) == len(embedding_cols), 'embedding_dims must be an integer or a list with the same length as embedding_cols'\n",
    "\n",
    "    # Numeric features.\n",
    "    for header in numeric_cols:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = get_normalization_layer(header, dataset)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        encoded_numeric_col = tf.keras.layers.Dropout(0.1)(encoded_numeric_col)\n",
    "        cont_inputs.append(numeric_col)\n",
    "        encoded_cont_features.append(encoded_numeric_col)\n",
    "\n",
    "    # Home and away teams\n",
    "    for i, header in enumerate(['home_team', 'away_team']):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(30, embedding_dims[i], name=f'{header}_embedding')(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    # Categorical features encoded as ints.\n",
    "    encoded_embedding_cols = embedding_cols.copy()\n",
    "    encoded_embedding_cols.remove('home_team')\n",
    "    encoded_embedding_cols.remove('away_team')\n",
    "    for i, header in enumerate(encoded_embedding_cols):\n",
    "        categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = get_category_encoding_layer(header, dataset, \n",
    "                                                     dtype='int', \n",
    "                                                     max_tokens=20)\n",
    "        encoded_categorical_col = encoding_layer(categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Embedding(20, 5, name=f'{header}_embedding')(encoded_categorical_col)\n",
    "        encoded_categorical_col = tf.keras.layers.Flatten()(encoded_categorical_col)\n",
    "        cat_inputs.append(categorical_col)\n",
    "        encoded_cat_features.append(encoded_categorical_col)\n",
    "        \n",
    "    all_inputs = cont_inputs + cat_inputs\n",
    "        \n",
    "    return all_inputs, encoded_cont_features, encoded_cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: ['pitcher_IP_pct_diff', 'team_FP_pct_diff', 'avg_pct_diff', 'team_W-L_pct_diff', 'WPA_pct_diff', 'win_pct_diff', 'ops_pct_diff', 'log_5', 'FP_pct_diff', 'team_WHIP_pct_diff', 'bayes_pct_diff', 'team_Rank_pct_diff', 'RA_pct_diff', 'pytha_pct_diff', 'RD_pct_diff', 'obp_pct_diff', 'R_pct_diff', 'team_ERA_pct_diff', 'slg_pct_diff', 'pitcher_WHIP_pct_diff', 'pitcher_ERA_pct_diff', 'Rank_pct_diff']\n",
      "Embedding home_team and away_team\n",
      "Categorical columns: ['Y', 'M']\n"
     ]
    }
   ],
   "source": [
    "all_inputs, encoded_cont_features, encoded_cat_features = prep_columns(train_ds,\n",
    "                                                                       10,\n",
    "                                                                       embedding_cols, \n",
    "                                                                       numeric_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenate(encoded_cat_features + encoded_cont_features)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "output = Dense(1)(x)\n",
    "del model\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "285/285 [==============================] - 4s 7ms/step - loss: 0.7246 - accuracy: 0.5544 - val_loss: 0.6648 - val_accuracy: 0.5947\n",
      "Epoch 2/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6638 - accuracy: 0.5801 - val_loss: 0.6616 - val_accuracy: 0.6071\n",
      "Epoch 3/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6491 - accuracy: 0.5883 - val_loss: 0.6578 - val_accuracy: 0.6054\n",
      "Epoch 4/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6455 - accuracy: 0.5915 - val_loss: 0.6561 - val_accuracy: 0.6055\n",
      "Epoch 5/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.5957 - val_loss: 0.6560 - val_accuracy: 0.6005\n",
      "Epoch 6/30\n",
      "285/285 [==============================] - 2s 7ms/step - loss: 0.6407 - accuracy: 0.5966 - val_loss: 0.6527 - val_accuracy: 0.6003\n",
      "Epoch 7/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6390 - accuracy: 0.6024 - val_loss: 0.6531 - val_accuracy: 0.5989\n",
      "Epoch 8/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6383 - accuracy: 0.6002 - val_loss: 0.6526 - val_accuracy: 0.5928\n",
      "Epoch 9/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6374 - accuracy: 0.6057 - val_loss: 0.6542 - val_accuracy: 0.5923\n",
      "Epoch 10/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6345 - accuracy: 0.6071 - val_loss: 0.6542 - val_accuracy: 0.5982\n",
      "Epoch 11/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6352 - accuracy: 0.6059 - val_loss: 0.6559 - val_accuracy: 0.5959\n",
      "Epoch 12/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6338 - accuracy: 0.6074 - val_loss: 0.6556 - val_accuracy: 0.5909\n",
      "Epoch 13/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6328 - accuracy: 0.6074 - val_loss: 0.6566 - val_accuracy: 0.5912\n",
      "Epoch 14/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6323 - accuracy: 0.6085 - val_loss: 0.6578 - val_accuracy: 0.5936\n",
      "Epoch 15/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6330 - accuracy: 0.6110 - val_loss: 0.6605 - val_accuracy: 0.5972\n",
      "Epoch 16/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6300 - accuracy: 0.6140 - val_loss: 0.6665 - val_accuracy: 0.6037\n",
      "Epoch 17/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6313 - accuracy: 0.6101 - val_loss: 0.6692 - val_accuracy: 0.6062\n",
      "Epoch 18/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6282 - accuracy: 0.6164 - val_loss: 0.6692 - val_accuracy: 0.5979\n",
      "Epoch 19/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6273 - accuracy: 0.6193 - val_loss: 0.6762 - val_accuracy: 0.6048\n",
      "Epoch 20/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6288 - accuracy: 0.6150 - val_loss: 0.6723 - val_accuracy: 0.6031\n",
      "Epoch 21/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6273 - accuracy: 0.6159 - val_loss: 0.6760 - val_accuracy: 0.6061\n",
      "Epoch 22/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6254 - accuracy: 0.6165 - val_loss: 0.6810 - val_accuracy: 0.6037\n",
      "Epoch 23/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6266 - accuracy: 0.6147 - val_loss: 0.6879 - val_accuracy: 0.6000\n",
      "Epoch 24/30\n",
      "285/285 [==============================] - 2s 5ms/step - loss: 0.6246 - accuracy: 0.6209 - val_loss: 0.6865 - val_accuracy: 0.6003\n",
      "Epoch 25/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6236 - accuracy: 0.6190 - val_loss: 0.6804 - val_accuracy: 0.6004\n",
      "Epoch 26/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6197 - val_loss: 0.6827 - val_accuracy: 0.5973\n",
      "Epoch 27/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6244 - accuracy: 0.6213 - val_loss: 0.6956 - val_accuracy: 0.5990\n",
      "Epoch 28/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6221 - accuracy: 0.6248 - val_loss: 0.6965 - val_accuracy: 0.6008\n",
      "Epoch 29/30\n",
      "285/285 [==============================] - 2s 6ms/step - loss: 0.6222 - accuracy: 0.6218 - val_loss: 0.7060 - val_accuracy: 0.5982\n",
      "Epoch 30/30\n",
      "285/285 [==============================] - 1s 5ms/step - loss: 0.6195 - accuracy: 0.6260 - val_loss: 0.7174 - val_accuracy: 0.6007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x151b7bd00>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 0s 3ms/step - loss: 0.7174 - accuracy: 0.6007\n",
      "Test accuracy = 60.07%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f'Test accuracy = {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
